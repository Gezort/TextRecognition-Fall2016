{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('SMSSpamCollection.txt', sep='\\t', names=['label', 'text'])\n",
    "data.label = data.label.apply(lambda x : 1 if x == 'spam' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5572.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.134063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.340751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             label\n",
       "count  5572.000000\n",
       "mean      0.134063\n",
       "std       0.340751\n",
       "min       0.000000\n",
       "25%       0.000000\n",
       "50%       0.000000\n",
       "75%       0.000000\n",
       "max       1.000000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      0  Go until jurong point, crazy.. Available only ...\n",
       "1      0                      Ok lar... Joking wif u oni...\n",
       "2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      0  U dun say so early hor... U c already then say...\n",
       "4      0  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "y_true = data.label\n",
    "X = vectorizer.fit_transform(data.text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.95890411  0.89855072  0.91549296  0.95833333  0.93706294  0.91304348\n",
      "  0.94444444  0.92753623  0.92198582  0.95104895]\n",
      "0.932640298361\n"
     ]
    }
   ],
   "source": [
    "log_reg_clf = LogisticRegression()\n",
    "cv_score = cross_val_score(log_reg_clf, X, y_true, cv=10, scoring='f1')\n",
    "print (cv_score)\n",
    "print (np.mean(cv_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_text = [\n",
    "\"FreeMsg: Txt: CALL to No: 86888 & claim your reward of 3 hours\\\n",
    " talk time to use from your phone now! Subscribe6GB\",\n",
    "\"FreeMsg: Txt: claim your reward of 3 hours talk time\",\n",
    "\"Have you visited the last lecture on physics?\",\n",
    "\"Have you visited the last lecture on physics? Just buy this book\\\n",
    " and you will have all materials! Only 99$\",\n",
    "\"Only 99$\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg_clf.fit(X, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_text = vectorizer.transform(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = log_reg_clf.predict(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 0 0 0\n"
     ]
    }
   ],
   "source": [
    "print (*y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.822422066419\n",
      "0.725016155547\n",
      "0.925138255865\n"
     ]
    }
   ],
   "source": [
    "for ngramm in [(2,2), (3,3), (1,3)]:\n",
    "    vectorizer.set_params(ngram_range=ngramm)\n",
    "    X = vectorizer.fit_transform(data.text.values)\n",
    "    cv_score = np.mean(cross_val_score(log_reg_clf, X, y_true, cv=10, scoring='f1'))\n",
    "    print (np.mean(cv_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.645501517799\n",
      "0.378719485246\n",
      "0.888485965606\n"
     ]
    }
   ],
   "source": [
    "nb_clf = MultinomialNB()\n",
    "for ngramm in [(2,2), (3,3), (1,3)]:\n",
    "    vectorizer.set_params(ngram_range=ngramm)\n",
    "    X = vectorizer.fit_transform(data.text.values)\n",
    "    cv_score = np.mean(cross_val_score(nb_clf, X, y_true, cv=10, scoring='f1'))\n",
    "    print (np.mean(cv_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.852859955417\n"
     ]
    }
   ],
   "source": [
    "tf_idf = TfidfVectorizer(ngram_range=(1,1))\n",
    "X = tf_idf.fit_transform(data.text.values)\n",
    "cv_score = np.mean(cross_val_score(log_reg_clf, X, y_true, cv=10, scoring='f1'))\n",
    "print (np.mean(cv_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Повышение качества на cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs_params = {\n",
    "    'penalty' : ['l1', 'l2'],\n",
    "    'C' : [0.1, 1., 10., 100.],\n",
    "    'fit_intercept' : [0, 1]\n",
    "}\n",
    "clf = LogisticRegression(max_iter=1000, random_state=427, n_jobs=-1, class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs = GridSearchCV(clf, gs_params, scoring='f1', cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1,3), stop_words='english')\n",
    "y_true = data.label\n",
    "X = vectorizer.fit_transform(data.text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=1000,\n",
       "          multi_class='ovr', n_jobs=-1, penalty='l2', random_state=427,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'penalty': ['l1', 'l2'], 'C': [0.1, 1.0, 10.0, 100.0], 'fit_intercept': [0, 1]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='f1', verbose=0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(X, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[mean: 0.83156, std: 0.02212, params: {'penalty': 'l1', 'C': 0.1, 'fit_intercept': 0},\n",
       " mean: 0.88337, std: 0.01090, params: {'penalty': 'l2', 'C': 0.1, 'fit_intercept': 0},\n",
       " mean: 0.87787, std: 0.01175, params: {'penalty': 'l1', 'C': 0.1, 'fit_intercept': 1},\n",
       " mean: 0.91963, std: 0.01566, params: {'penalty': 'l2', 'C': 0.1, 'fit_intercept': 1},\n",
       " mean: 0.88028, std: 0.01165, params: {'penalty': 'l1', 'C': 1.0, 'fit_intercept': 0},\n",
       " mean: 0.90772, std: 0.00960, params: {'penalty': 'l2', 'C': 1.0, 'fit_intercept': 0},\n",
       " mean: 0.92579, std: 0.01025, params: {'penalty': 'l1', 'C': 1.0, 'fit_intercept': 1},\n",
       " mean: 0.92143, std: 0.01380, params: {'penalty': 'l2', 'C': 1.0, 'fit_intercept': 1},\n",
       " mean: 0.90034, std: 0.01317, params: {'penalty': 'l1', 'C': 10.0, 'fit_intercept': 0},\n",
       " mean: 0.89486, std: 0.00939, params: {'penalty': 'l2', 'C': 10.0, 'fit_intercept': 0},\n",
       " mean: 0.92421, std: 0.00545, params: {'penalty': 'l1', 'C': 10.0, 'fit_intercept': 1},\n",
       " mean: 0.91729, std: 0.01436, params: {'penalty': 'l2', 'C': 10.0, 'fit_intercept': 1},\n",
       " mean: 0.90302, std: 0.00803, params: {'penalty': 'l1', 'C': 100.0, 'fit_intercept': 0},\n",
       " mean: 0.88065, std: 0.00855, params: {'penalty': 'l2', 'C': 100.0, 'fit_intercept': 0},\n",
       " mean: 0.93705, std: 0.00486, params: {'penalty': 'l1', 'C': 100.0, 'fit_intercept': 1},\n",
       " mean: 0.91422, std: 0.01181, params: {'penalty': 'l2', 'C': 100.0, 'fit_intercept': 1}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'C': 100.0, 'fit_intercept': 1, 'penalty': 'l1'}, 0.93704928903258045)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_, gs.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В задачах, связанных с распознаваниями текстов, лучше делать некую фильтрацию признаков (благодаря этому мы сможем избавиться от очень частых или очень редких слов, например). Если использовать в качестве признаков не сами слова, а n-граммы, это дает как правило лучшее качество, т.к. изначально мы имеем больше информации о том, какие слова встречаются рядом, например."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
