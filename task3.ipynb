{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning for Natural Language Processing\n",
    "\n",
    "\n",
    " * Simple text representations, bag of words\n",
    " * Word embedding and... not just another word2vec this time\n",
    " * rnn for text\n",
    " * Aggregating several data sources \"the hard way\"\n",
    " * Solving ~somewhat~ real ML problem with ~almost~ end-to-end deep learning\n",
    " \n",
    "\n",
    "Special thanks to Irina Golzmann for help with technical part, task prepared by Александр Панин, jheuristic@yandex-team.ru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK\n",
    "\n",
    "You will require nltk v3.2 to solve this assignment\n",
    "\n",
    "__It is really important that the version is 3.2, otherwize russian tokenizer might not work__\n",
    "\n",
    "Install/update\n",
    "* `sudo pip install --upgrade nltk==3.2`\n",
    "* If you don't remember when was the last pip upgrade, `sudo pip install --upgrade pip`\n",
    "\n",
    "If for some reason you can't or won't switch to nltk v3.2, just make sure that russian words are tokenized properly with RegeExpTokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For students with low-RAM machines\n",
    " * This assignment can be accomplished with even the low-tier hardware (<= 4Gb RAM) \n",
    " * If that is the case, turn flag \"low_RAM_mode\" below to True\n",
    " * If you have around 8GB memory, it is unlikely that you will feel constrained by memory.\n",
    " * In case you are using a PC from last millenia, consider setting very_low_RAM=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "low_RAM_mode = True\n",
    "very_low_RAM = False  #If you have <3GB RAM, set BOTH to true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "Ex-kaggle-competition on prohibited content detection\n",
    "\n",
    "There goes the description - https://www.kaggle.com/c/avito-prohibited-content\n",
    "\n",
    "\n",
    "### Download\n",
    "High-RAM mode,\n",
    " * Download avito_train.tsv from competition data files\n",
    "Low-RAM-mode,\n",
    " * Download downsampled dataset from here\n",
    "     * archive https://yadi.sk/d/l0p4lameqw3W8\n",
    "     * raw https://yadi.sk/d/I1v7mZ6Sqw2WK (in case you feel masochistic)\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# What's inside\n",
    "Different kinds of features:\n",
    "* 2 text fields - title and description\n",
    "* Special features - price, number of e-mails, phones, etc\n",
    "* Category and subcategory - unsurprisingly, categorical features\n",
    "* Attributes - more factors\n",
    "\n",
    "Only 1 binary target whether or not such advertisement contains prohibited materials\n",
    "* criminal, misleading, human reproduction-related, etc\n",
    "* diving into the data may result in prolonged sleep disorders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not low_RAM_mode:\n",
    "    # a lot of ram\n",
    "    df = pd.read_csv(\"avito_train.tsv\",sep='\\t')\n",
    "else:\n",
    "    #around 4GB ram\n",
    "    df = pd.read_csv(\"avito_train_1kk.tsv\",sep='\\t')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1204949, 13) 0.228222107326\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemid</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>attrs</th>\n",
       "      <th>price</th>\n",
       "      <th>is_proved</th>\n",
       "      <th>is_blocked</th>\n",
       "      <th>phones_cnt</th>\n",
       "      <th>emails_cnt</th>\n",
       "      <th>urls_cnt</th>\n",
       "      <th>close_hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000010</td>\n",
       "      <td>Транспорт</td>\n",
       "      <td>Автомобили с пробегом</td>\n",
       "      <td>Toyota Sera, 1991</td>\n",
       "      <td>Новая оригинальная линзованая оптика на ксенон...</td>\n",
       "      <td>{\"Год выпуска\":\"1991\", \"Тип кузова\":\"Купе\", \"П...</td>\n",
       "      <td>150000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000094</td>\n",
       "      <td>Личные вещи</td>\n",
       "      <td>Одежда, обувь, аксессуары</td>\n",
       "      <td>Костюм Steilmann</td>\n",
       "      <td>Юбка и топ из панбархата. Под топ  трикотажная...</td>\n",
       "      <td>{\"Вид одежды\":\"Женская одежда\", \"Предмет одежд...</td>\n",
       "      <td>1500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000299</td>\n",
       "      <td>Личные вещи</td>\n",
       "      <td>Детская одежда и обувь</td>\n",
       "      <td>Костюм Didriksons Boardman, размер 100, краги,...</td>\n",
       "      <td>Костюм Didriksons Boardman, в отличном состоян...</td>\n",
       "      <td>{\"Вид одежды\":\"Для мальчиков\", \"Предмет одежды...</td>\n",
       "      <td>3000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000309</td>\n",
       "      <td>Недвижимость</td>\n",
       "      <td>Квартиры</td>\n",
       "      <td>1-к квартира, 44 м², 9/20 эт.</td>\n",
       "      <td>В кирпичном пан.-м доме, продается одноком.-ая...</td>\n",
       "      <td>{\"Тип объявления\":\"Продам\", \"Количество комнат...</td>\n",
       "      <td>2642020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000317</td>\n",
       "      <td>Услуги</td>\n",
       "      <td>Предложения услуг</td>\n",
       "      <td>Поездки на таможню, печать в паспорте</td>\n",
       "      <td>Поездки на таможню гражданам СНГ для пересечен...</td>\n",
       "      <td>{\"Вид услуги\":\"Деловые услуги\", \"Тип услуги\":\"...</td>\n",
       "      <td>1500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     itemid      category                subcategory  \\\n",
       "0  10000010     Транспорт      Автомобили с пробегом   \n",
       "1  10000094   Личные вещи  Одежда, обувь, аксессуары   \n",
       "2  10000299   Личные вещи     Детская одежда и обувь   \n",
       "3  10000309  Недвижимость                   Квартиры   \n",
       "4  10000317        Услуги          Предложения услуг   \n",
       "\n",
       "                                               title  \\\n",
       "0                                  Toyota Sera, 1991   \n",
       "1                                   Костюм Steilmann   \n",
       "2  Костюм Didriksons Boardman, размер 100, краги,...   \n",
       "3                      1-к квартира, 44 м², 9/20 эт.   \n",
       "4              Поездки на таможню, печать в паспорте   \n",
       "\n",
       "                                         description  \\\n",
       "0  Новая оригинальная линзованая оптика на ксенон...   \n",
       "1  Юбка и топ из панбархата. Под топ  трикотажная...   \n",
       "2  Костюм Didriksons Boardman, в отличном состоян...   \n",
       "3  В кирпичном пан.-м доме, продается одноком.-ая...   \n",
       "4  Поездки на таможню гражданам СНГ для пересечен...   \n",
       "\n",
       "                                               attrs    price  is_proved  \\\n",
       "0  {\"Год выпуска\":\"1991\", \"Тип кузова\":\"Купе\", \"П...   150000        NaN   \n",
       "1  {\"Вид одежды\":\"Женская одежда\", \"Предмет одежд...     1500        NaN   \n",
       "2  {\"Вид одежды\":\"Для мальчиков\", \"Предмет одежды...     3000        NaN   \n",
       "3  {\"Тип объявления\":\"Продам\", \"Количество комнат...  2642020        NaN   \n",
       "4  {\"Вид услуги\":\"Деловые услуги\", \"Тип услуги\":\"...     1500        0.0   \n",
       "\n",
       "   is_blocked  phones_cnt  emails_cnt  urls_cnt  close_hours  \n",
       "0           0           0           0         0         0.03  \n",
       "1           0           0           0         0         0.41  \n",
       "2           0           0           0         0         5.49  \n",
       "3           0           1           0         0        22.47  \n",
       "4           1           0           0         0         1.43  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (df.shape, df.is_blocked.mean())\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](https://kaggle2.blob.core.windows.net/competitions/kaggle/3929/media/Ad.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blocked ratio 0.228222107326\n",
      "Count: 1204949\n"
     ]
    }
   ],
   "source": [
    "print (\"Blocked ratio\",df.is_blocked.mean())\n",
    "print (\"Count:\",len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balance-out the classes\n",
    "* Vast majority of data samples are non-prohibited\n",
    " * 250k banned out of 4kk\n",
    " * Let's just downsample random 250k legal samples to make further steps less computationally demanding\n",
    " * If you aim for high Kaggle score, consider a smarter approach to that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(category\n",
       " Бытовая электроника    0.074260\n",
       " Для бизнеса            0.490611\n",
       " Для дома и дачи        0.112549\n",
       " Животные               0.103441\n",
       " Личные вещи            0.139700\n",
       " Недвижимость           0.004959\n",
       " Работа                 0.255127\n",
       " Транспорт              0.097432\n",
       " Услуги                 0.718983\n",
       " Хобби и отдых          0.619468\n",
       " Name: is_blocked, dtype: float64,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['category'])['is_blocked'].mean(),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Недвижимость           287778\n",
       "Транспорт              185114\n",
       "Услуги                 173025\n",
       "Личные вещи            167788\n",
       "Бытовая электроника    101131\n",
       "Хобби и отдых           92142\n",
       "Работа                  77777\n",
       "Для дома и дачи         61298\n",
       "Животные                32376\n",
       "Для бизнеса             26520\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#downsample\n",
    "data = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = data[data.is_blocked == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data[data.is_blocked == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(274996, 13)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blocked ratio: 0.5\n",
      "Count: 549992\n"
     ]
    }
   ],
   "source": [
    "for category in data.category.value_counts().keys():\n",
    "    to_add = data[data.category == category]\n",
    "    n_samples = df.category.value_counts()[category]\n",
    "    indexes = np.random.choice(np.arange(to_add.shape[0]), size=n_samples)\n",
    "    df = df.append(to_add.iloc[indexes])\n",
    "\n",
    "print (\"Blocked ratio:\",df.is_blocked.mean())\n",
    "print (\"Count:\",len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "Бытовая электроника    0.5\n",
       "Для бизнеса            0.5\n",
       "Для дома и дачи        0.5\n",
       "Животные               0.5\n",
       "Личные вещи            0.5\n",
       "Недвижимость           0.5\n",
       "Работа                 0.5\n",
       "Транспорт              0.5\n",
       "Услуги                 0.5\n",
       "Хобби и отдых          0.5\n",
       "Name: is_blocked, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('category')['is_blocked'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed\n"
     ]
    }
   ],
   "source": [
    "assert df.is_blocked.mean() < 0.51\n",
    "assert df.is_blocked.mean() > 0.49\n",
    "assert len(df) <= 560000\n",
    "\n",
    "print (\"All tests passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#In case your RAM-o-meter is in the red\n",
    "if very_low_RAM:\n",
    "    df = df[::2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Tokenizing\n",
    "\n",
    "First, we create a dictionary of all existing words.\n",
    "Assign each word a number - it's Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter,defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "#Dictionary of tokens\n",
    "token_counts = Counter()\n",
    "\n",
    "#All texts\n",
    "all_texts = np.hstack([df.description.values,df.title.values])\n",
    "\n",
    "\n",
    "#Compute token frequencies\n",
    "for s in all_texts:\n",
    "    if type(s) is not str:\n",
    "        continue\n",
    "    s = s.lower()\n",
    "    tokens = tokenizer.tokenize(s)\n",
    "    for token in tokens:\n",
    "        token_counts[token] +=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove rare tokens\n",
    "\n",
    "We are unlikely to make use of words that are only seen a few times throughout the corpora.\n",
    "\n",
    "Again, if you want to beat Kaggle competition metrics, consider doing something better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([      0.,  181759.,   66045.,   35527.,   24935.,   16416.,\n",
       "          13022.,    9263.,    7590.,    6200.,    5565.,    4463.,\n",
       "           4134.,    3532.,    3110.,    2780.,    2573.,    2280.,\n",
       "           2160.,    2040.,    1780.,    1628.,    1600.,    1406.,\n",
       "           1363.,    1224.,    1194.,    1191.,    1132.,    1004.,\n",
       "            905.,     896.,     949.,     803.,     780.,     738.,\n",
       "            717.,     697.,     693.,     652.,     593.,     565.,\n",
       "            580.,     568.,     489.,     489.,     517.,     465.,\n",
       "            483.,     862.]),\n",
       " array([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,\n",
       "         11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,\n",
       "         22.,  23.,  24.,  25.,  26.,  27.,  28.,  29.,  30.,  31.,  32.,\n",
       "         33.,  34.,  35.,  36.,  37.,  38.,  39.,  40.,  41.,  42.,  43.,\n",
       "         44.,  45.,  46.,  47.,  48.,  49.,  50.]),\n",
       " <a list of 50 Patch objects>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAicAAAFkCAYAAAD7dJuCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8c3e66d438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Word frequency distribution, just for kicks\n",
    "plt.hist(list(token_counts.values()),range=[0,50],bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Select only the tokens that had at least 10 occurences in the corpora.\n",
    "#Use token_counts.\n",
    "\n",
    "min_count = 10\n",
    "tokens = dict(token for token in list(token_counts.items()) if token[1] > min_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_to_id = {t:i+1 for i,t in enumerate(tokens)}\n",
    "null_token = \"NULL\"\n",
    "token_to_id[null_token] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tokens: 86912\n"
     ]
    }
   ],
   "source": [
    "print (\"# Tokens:\",len(token_to_id))\n",
    "if len(token_to_id) < 30000:\n",
    "    print (\"Alarm! It seems like there are too few tokens. Make sure you updated NLTK and applied correct thresholds -- unless you now what you're doing, ofc\")\n",
    "if len(token_to_id) > 1000000:\n",
    "    print (\"Alarm! Too many tokens. You might have messed up when pruning rare ones -- unless you know what you're doin' ofc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace words with IDs\n",
    "Set a maximum length for titles and descriptions.\n",
    " * If string is longer that that limit - crop it, if less - pad with zeros.\n",
    " * Thus we obtain a matrix of size [n_samples]x[max_length]\n",
    " * Element at i,j - is an identifier of word j within sample i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize(strings, token_to_id, max_len=150):\n",
    "    token_matrix = []\n",
    "    for s in strings:\n",
    "        if type(s) is not str:\n",
    "            token_matrix.append([0]*max_len)\n",
    "            continue\n",
    "        s = s.lower()\n",
    "        tokens = tokenizer.tokenize(s)\n",
    "        token_ids = list(map(lambda token: token_to_id.get(token,0), tokens))[:max_len]\n",
    "        token_ids += [0]*(max_len - len(token_ids))\n",
    "        token_matrix.append(token_ids)\n",
    "\n",
    "    return np.array(token_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "desc_tokens = vectorize(df.description.values,token_to_id,max_len = 150)\n",
    "title_tokens = vectorize(df.title.values,token_to_id,max_len = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Data format examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер матрицы: (549992, 15)\n",
      "Поездки на таможню, печать в паспорте -> [64436    46 60115 68789 50307 58830     0     0     0     0] ...\n",
      "Рефлекторно-урогинекологический массаж -> [62202     0 72403     0     0     0     0     0     0     0] ...\n",
      "Возьму суду под200 т. р -> [18664 66141     0 49472 51408     0     0     0     0     0] ...\n"
     ]
    }
   ],
   "source": [
    "print (\"Размер матрицы:\",title_tokens.shape)\n",
    "for title, tokens in zip(df.title.values[:3],title_tokens[:3]):\n",
    "    print (title,'->', tokens[:10],'...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ As you can see, our preprocessing is somewhat crude. Let us see if that is enough for our network __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-sequences\n",
    "\n",
    "\n",
    "Some data features are not text samples. E.g. price, # urls, category, etc\n",
    "\n",
    "They require a separate preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#All numeric features\n",
    "df_numerical_features = df[[\"phones_cnt\",\"emails_cnt\",\"urls_cnt\",\"price\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#One-hot-encoded category and subcategory\n",
    "\n",
    "categories = []\n",
    "data_cat_subcat = df[[\"category\",\"subcategory\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categories = [{\"category\":cat_name, \"subcategory\":subcat_name} for cat_name, subcat_name in data_cat_subcat]\n",
    "\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "cat_one_hot = vectorizer.fit_transform(categories)\n",
    "cat_one_hot = pd.DataFrame(cat_one_hot,columns=vectorizer.feature_names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_non_text = pd.merge(\n",
    "    df_numerical_features,cat_one_hot,on = np.arange(len(cat_one_hot))\n",
    ")\n",
    "del df_non_text[\"key_0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Target variable - whether or not sample contains prohibited material\n",
    "target = df.is_blocked.values.astype('int32')\n",
    "#Preprocessed titles\n",
    "title_tokens = title_tokens.astype('int32')\n",
    "#Preprocessed tokens\n",
    "desc_tokens = desc_tokens.astype('int32')\n",
    "\n",
    "#Non-sequences\n",
    "df_non_text = df_non_text.astype('float32').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Split into training and test set.\n",
    "\n",
    "#Difficulty selector:\n",
    "#Easy: split randomly\n",
    "#Medium: select test set items that have item_ids strictly above that of training set\n",
    "#Hard: do whatever you want, but score yourself using kaggle private leaderboard\n",
    "\n",
    "data_tuple = train_test_split(title_tokens, desc_tokens, df_non_text, target, test_size=0.2, random_state=123)\n",
    "title_tr,title_ts,desc_tr,desc_ts,nontext_tr,nontext_ts,target_tr,target_ts = data_tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save preprocessed data [optional]\n",
    "\n",
    "* The next tab can be used to stash all the essential data matrices and get rid of the rest of the data.\n",
    " * Highly recommended if you have less than 1.5GB RAM left\n",
    "* To do that, you need to first run it with save_prepared_data=True, then restart the notebook and only run this tab with read_prepared_data=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading saved data...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "save_prepared_data = False #save\n",
    "read_prepared_data = True #load\n",
    "\n",
    "#but not both at once\n",
    "assert not (save_prepared_data and read_prepared_data)\n",
    "\n",
    "if save_prepared_data:\n",
    "    print (\"Saving preprocessed data (may take up to 3 minutes)\")\n",
    "\n",
    "    import pickle\n",
    "    with open(\"preprocessed_data.pcl\",'wb') as fout:\n",
    "        pickle.dump(data_tuple, fout)\n",
    "    with open(\"token_to_id.pcl\",'wb') as fout:\n",
    "        pickle.dump(token_to_id,fout)\n",
    "\n",
    "    print (\"готово\")\n",
    "    \n",
    "elif read_prepared_data:\n",
    "    print (\"Reading saved data...\")\n",
    "    \n",
    "    import pickle\n",
    "    \n",
    "    with open(\"preprocessed_data.pcl\",'rb') as fin:\n",
    "        data_tuple = pickle.load(fin)\n",
    "    title_tr,title_ts,desc_tr,desc_ts,nontext_tr,nontext_ts,target_tr,target_ts = data_tuple\n",
    "    with open(\"token_to_id.pcl\",'rb') as fin:\n",
    "        token_to_id = pickle.load(fin)\n",
    "        \n",
    "    #Re-importing libraries to allow staring noteboook from here\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "   \n",
    "    print (\"done\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the monster\n",
    "\n",
    "Since we have several data sources, our neural network may differ from what you used to work with.\n",
    "\n",
    "* Separate input for titles: RNN\n",
    "* Separate input for description: RNN\n",
    "* Separate input for categorical features: обычные полносвязные слои или какие-нибудь трюки\n",
    " \n",
    "These three inputs must be blended somehow - concatenated or added.\n",
    "\n",
    "* Output: a simple binary classification\n",
    " * 1 sigmoidal with binary_crossentropy\n",
    " * 2 softmax with categorical_crossentropy - essentially the same as previous one\n",
    " * 1 neuron without nonlinearity (lambda x: x) +  hinge loss\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#libraries\n",
    "import lasagne\n",
    "from theano import tensor as T\n",
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#3 inputs and a refere output\n",
    "title_token_ids = T.matrix(\"title_token_ids\",dtype='int32')\n",
    "desc_token_ids = T.matrix(\"desc_token_ids\",dtype='int32')\n",
    "categories = T.matrix(\"categories\",dtype='float32')\n",
    "target_y = T.ivector(\"is_blocked\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "title_inp = lasagne.layers.InputLayer((None,title_tr.shape[1]),input_var=title_token_ids)\n",
    "descr_inp = lasagne.layers.InputLayer((None,desc_tr.shape[1]),input_var=desc_token_ids)\n",
    "cat_inp = lasagne.layers.InputLayer((None,nontext_tr.shape[1]), input_var=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Descriptions\n",
    "\n",
    "#word-wise embedding. We recommend to start from some 64 and improving after you are certain it works.\n",
    "descr_nn = lasagne.layers.EmbeddingLayer(descr_inp, input_size=len(token_to_id)+1, output_size=128)\n",
    "descr_nn = lasagne.layers.LSTMLayer(descr_nn, 25, only_return_final=True)\n",
    "\n",
    "# Titles\n",
    "title_nn = lasagne.layers.EmbeddingLayer(title_inp,input_size=len(token_to_id)+1, output_size=128)\n",
    "title_nn = lasagne.layers.Conv1DLayer(title_nn, num_filters=50, filter_size=5)\n",
    "title_nn = lasagne.layers.MaxPool1DLayer(title_nn, pool_size=2)\n",
    "title_nn = lasagne.layers.DenseLayer(title_nn, num_units=64)\n",
    "\n",
    "# Non-sequences\n",
    "cat_nn = lasagne.layers.DenseLayer(cat_inp, num_units=128)\n",
    "cat_nn = lasagne.layers.DenseLayer(cat_inp, num_units=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn = lasagne.layers.concat([descr_nn, title_nn, cat_nn])\n",
    "\n",
    "nn = lasagne.layers.DenseLayer(nn, 1024)\n",
    "nn = lasagne.layers.DropoutLayer(nn, p=0.2)\n",
    "nn = lasagne.layers.DenseLayer(nn, 1, nonlinearity=lasagne.nonlinearities.linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function\n",
    "\n",
    "* The standard way:\n",
    " * prediction\n",
    " * loss\n",
    " * updates\n",
    " * training and evaluation functions\n",
    " \n",
    " \n",
    "* Hinge loss\n",
    " * $ L_i = \\max(0, \\delta - t_i p_i) $\n",
    " * delta is a tunable parameter: how far should a neuron be in the positive margin area for us to stop bothering about it\n",
    " * Function description may mention some +-1  limitations - this is not neccessary, at least as long as hinge loss has a __default__ flag `binary = True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#All trainable params\n",
    "weights = lasagne.layers.get_all_params(nn, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/lasagne/layers/pool.py:162: UserWarning: DEPRECATION: the 'ds' parameter is not going to exist anymore as it is going to be replaced by the parameter 'ws'.\n",
      "  mode=self.mode,\n",
      "/usr/local/lib/python3.5/dist-packages/lasagne/layers/pool.py:162: UserWarning: DEPRECATION: the 'st' parameter is not going to exist anymore as it is going to be replaced by the parameter 'stride'.\n",
      "  mode=self.mode,\n",
      "/usr/local/lib/python3.5/dist-packages/lasagne/layers/pool.py:162: UserWarning: DEPRECATION: the 'padding' parameter is not going to exist anymore as it is going to be replaced by the parameter 'pad'.\n",
      "  mode=self.mode,\n"
     ]
    }
   ],
   "source": [
    "#Simple NN prediction\n",
    "train_prediction = lasagne.layers.get_output(nn)\n",
    "\n",
    "#Hinge loss\n",
    "train_loss = lasagne.objectives.binary_hinge_loss(train_prediction, target_y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Weight optimization step\n",
    "updates = lasagne.updates.adam(train_loss, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determinitic prediction \n",
    " * In case we use stochastic elements, e.g. dropout or noize\n",
    " * Compile a separate set of functions with deterministic prediction (deterministic = True)\n",
    " * Unless you think there's no neet for dropout there ofc. Btw is there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/lasagne/layers/pool.py:162: UserWarning: DEPRECATION: the 'ds' parameter is not going to exist anymore as it is going to be replaced by the parameter 'ws'.\n",
      "  mode=self.mode,\n",
      "/usr/local/lib/python3.5/dist-packages/lasagne/layers/pool.py:162: UserWarning: DEPRECATION: the 'st' parameter is not going to exist anymore as it is going to be replaced by the parameter 'stride'.\n",
      "  mode=self.mode,\n",
      "/usr/local/lib/python3.5/dist-packages/lasagne/layers/pool.py:162: UserWarning: DEPRECATION: the 'padding' parameter is not going to exist anymore as it is going to be replaced by the parameter 'pad'.\n",
      "  mode=self.mode,\n"
     ]
    }
   ],
   "source": [
    "#deterministic version\n",
    "test_prediction = lasagne.layers.get_output(nn, deterministic=True)\n",
    "\n",
    "#equivalent loss function\n",
    "test_loss = lasagne.objectives.binary_hinge_loss(test_prediction, target_y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coffee-lation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_fun = theano.function([desc_token_ids,title_token_ids,categories,target_y],\n",
    "                                            [train_loss, train_prediction],updates = updates)\n",
    "eval_fun = theano.function([desc_token_ids,title_token_ids,categories,target_y], [test_loss, test_prediction])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop\n",
    "* The regular way with loops over minibatches\n",
    "* Since the dataset is huge, we define epoch as some fixed amount of samples isntead of all dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#average precision at K\n",
    "from oracle import APatK, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Out good old minibatch iterator now supports arbitrary amount of arrays (X,y,z)\n",
    "\n",
    "def iterate_minibatches(*arrays,**kwargs):\n",
    "    batchsize=kwargs.get(\"batchsize\",100)\n",
    "    shuffle = kwargs.get(\"shuffle\",True)\n",
    "    \n",
    "    if shuffle:\n",
    "        indices = np.arange(len(arrays[0]))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(arrays[0]) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield [arr[excerpt] for arr in arrays]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweaking guide\n",
    "\n",
    "* batch_size - how many samples are processed per function call\n",
    "  * optimization gets slower, but more stable, as you increase it.\n",
    "  * May consider increasing it halfway through training\n",
    "* minibatches_per_epoch - max amount of minibatches per epoch\n",
    "  * Does not affect training. Lesser value means more frequent and less stable printing\n",
    "  * Setting it to less than 10 is only meaningfull if you want to make sure your NN does not break down after one epoch\n",
    "* n_epochs - total amount of epochs to train for\n",
    "  * `n_epochs = 10**10` and manual interrupting is still an option\n",
    "\n",
    "\n",
    "Tips:\n",
    "\n",
    "* With small minibatches_per_epoch, network quality may jump around 0.5 for several epochs\n",
    "\n",
    "* AUC is the most stable of all three metrics\n",
    "\n",
    "* Average Precision at top 2.5% (APatK) - is the least stable. If batch_size*minibatches_per_epoch < 10k, it behaves as a uniform random variable.\n",
    "\n",
    "* Plotting metrics over training time may be a good way to analyze which architectures work better.\n",
    "\n",
    "* Once you are sure your network aint gonna crash, it's worth letting it train for a few hours of an average laptop's time to see it's true potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "\tloss: 0.165661929106\n",
      "\tacc: 0.928514851485\n",
      "\tauc: 0.966459765539\n",
      "\tap@k: 0.997755113677\n",
      "Val:\n",
      "\tloss: 0.20119328977\n",
      "\tacc: 0.909405940594\n",
      "\tauc: 0.959296764504\n",
      "\tap@k: 0.972366118065\n",
      "Train:\n",
      "\tloss: 0.169086945436\n",
      "\tacc: 0.92495049505\n",
      "\tauc: 0.969429777222\n",
      "\tap@k: 0.978498588057\n",
      "Val:\n",
      "\tloss: 0.21468896032\n",
      "\tacc: 0.905742574257\n",
      "\tauc: 0.957040215737\n",
      "\tap@k: 0.999589614771\n",
      "Train:\n",
      "\tloss: 0.172801756824\n",
      "\tacc: 0.921881188119\n",
      "\tauc: 0.966253646478\n",
      "\tap@k: 0.973482181334\n",
      "Val:\n",
      "\tloss: 0.214727070805\n",
      "\tacc: 0.904356435644\n",
      "\tauc: 0.951327834944\n",
      "\tap@k: 0.947422051766\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "n_epochs = 3\n",
    "batch_size = 100\n",
    "minibatches_per_epoch = 100\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    #training\n",
    "    epoch_y_true = []\n",
    "    epoch_y_pred = []\n",
    "    \n",
    "    b_c = b_loss = 0\n",
    "    for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n",
    "        iterate_minibatches(desc_tr,title_tr,nontext_tr,target_tr,batchsize=batch_size,shuffle=True)):\n",
    "        if j > minibatches_per_epoch:break\n",
    "            \n",
    "        loss,pred_probas = train_fun(b_desc,b_title,b_cat,b_y)\n",
    "        \n",
    "        b_loss += loss\n",
    "        b_c +=1\n",
    "        \n",
    "        epoch_y_true.append(b_y)\n",
    "        epoch_y_pred.append(pred_probas)\n",
    "    \n",
    "    epoch_y_true = np.concatenate(epoch_y_true)\n",
    "    epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "    \n",
    "    print (\"Train:\")\n",
    "    print ('\\tloss:',b_loss/b_c)\n",
    "    print ('\\tacc:',accuracy_score(epoch_y_true,epoch_y_pred>0.))\n",
    "    print ('\\tauc:',roc_auc_score(epoch_y_true,epoch_y_pred))\n",
    "    print ('\\tap@k:',APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1))\n",
    "    \n",
    "    #evaluation\n",
    "    epoch_y_true = []\n",
    "    epoch_y_pred = []\n",
    "    b_c = b_loss = 0\n",
    "    for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n",
    "        iterate_minibatches(desc_ts,title_ts,nontext_tr,target_ts,batchsize=batch_size,shuffle=True)):\n",
    "        if j > minibatches_per_epoch: break\n",
    "        loss,pred_probas = eval_fun(b_desc,b_title,b_cat,b_y)\n",
    "        \n",
    "        b_loss += loss\n",
    "        b_c +=1\n",
    "        \n",
    "        epoch_y_true.append(b_y)\n",
    "        epoch_y_pred.append(pred_probas)\n",
    "\n",
    "    epoch_y_true = np.concatenate(epoch_y_true)\n",
    "    epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "    \n",
    "    print (\"Val:\")\n",
    "    print ('\\tloss:',b_loss/b_c)\n",
    "    print ('\\tacc:',accuracy_score(epoch_y_true,epoch_y_pred>0.))\n",
    "    print ('\\tauc:',roc_auc_score(epoch_y_true,epoch_y_pred))\n",
    "    print ('\\tap@k:',APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final evaluation\n",
    "Evaluate network over the entire test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:\n",
      "\tloss: 0.215000746387\n",
      "\tacc: 0.903175614195\n",
      "\tauc: 0.950800880543\n",
      "\tap@k: 0.945807488125\n",
      "\n",
      "AUC:\n",
      "\tСойдёт, хотя можно ещё поднажать (ok)\n",
      "\n",
      "Accuracy:\n",
      "\tВсё ок (ok)\n",
      "\n",
      "Average precision at K:\n",
      "\tВы побили baseline (ok)\n"
     ]
    }
   ],
   "source": [
    "#evaluation\n",
    "epoch_y_true = []\n",
    "epoch_y_pred = []\n",
    "\n",
    "b_c = b_loss = 0\n",
    "for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n",
    "    iterate_minibatches(desc_ts,title_ts,nontext_tr,target_ts,batchsize=batch_size,shuffle=True)):\n",
    "    loss,pred_probas = eval_fun(b_desc,b_title,b_cat,b_y)\n",
    "\n",
    "    b_loss += loss\n",
    "    b_c +=1\n",
    "\n",
    "    epoch_y_true.append(b_y)\n",
    "    epoch_y_pred.append(pred_probas)\n",
    "\n",
    "\n",
    "epoch_y_true = np.concatenate(epoch_y_true)\n",
    "epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "\n",
    "final_accuracy = accuracy_score(epoch_y_true,epoch_y_pred>0)\n",
    "final_auc = roc_auc_score(epoch_y_true,epoch_y_pred)\n",
    "final_apatk = APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1)\n",
    "\n",
    "print (\"Scores:\")\n",
    "print ('\\tloss:',b_loss/b_c)\n",
    "print ('\\tacc:',final_accuracy)\n",
    "print ('\\tauc:',final_auc)\n",
    "print ('\\tap@k:',final_apatk)\n",
    "score(final_accuracy,final_auc,final_apatk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main task\n",
    "\n",
    "* https://goo.gl/forms/eJwIeAbjxzVuo6vn1\n",
    "* Feel like Le'Cun:\n",
    " * accuracy > 0.95\n",
    " * AUC > 0.97\n",
    " * Average Precision at (test sample size * 0.025) > 0.99\n",
    " * And perhaps even farther\n",
    "\n",
    "* Casual mode\n",
    " * accuracy > 0.90\n",
    " * AUC > 0.95\n",
    " * Average Precision at (test sample size * 0.025) > 0.92\n",
    "\n",
    "* Remember the training, Luke\n",
    " * Dropout, regularization\n",
    " * Mommentum, RMSprop, ada*\n",
    " * etc etc etc\n",
    " \n",
    " * If you have background in texts, there may be a way to improve tokenizer, add some lemmatization, etc etc.\n",
    " * In case you know how not to shoot yourself in the foot with RNNs, they too may be of some use."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
